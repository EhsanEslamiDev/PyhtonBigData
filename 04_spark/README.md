# Module 4: Apache Spark with PySpark

## Overview
This module covers big data processing using Apache Spark and PySpark.

## Topics Covered
1. Spark fundamentals and architecture
2. RDD (Resilient Distributed Datasets) operations
3. DataFrames and Datasets
4. Spark SQL
5. Performance optimization
6. Streaming data processing

## Files in this module:
- `01_spark_basics.py` - Spark fundamentals
- `02_rdd_operations.py` - RDD transformations and actions
- `03_dataframes.py` - DataFrame operations
- `04_spark_sql.py` - SQL operations in Spark
- `05_optimization.py` - Performance tuning

## Prerequisites
- Java 8 or higher installed
- Apache Spark installed (included in requirements.txt)

## Learning Objectives
- Understand Spark architecture and concepts
- Master RDD operations for distributed computing
- Work with Spark DataFrames and SQL
- Optimize Spark applications for performance